# backend/ask_cli.py

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # âœ… Suppress tokenizer fork warning

from llm_setup import get_qa_chain

def main():
    print("ğŸ‘‹ Welcome to the Role-Based QA Assistant")
    user_role = input("ğŸ” Enter your role (e.g., finance, hr, engineering): ").strip().lower()

    qa_chain = get_qa_chain(user_role)

    print("\nğŸ’¬ Ask any question related to your role documents (type 'exit' to quit):")
    while True:
        query = input("\nğŸ§  You: ")
        if query.lower() in {"exit", "quit"}:
            break

        result = qa_chain.invoke(query)

        print(f"\nğŸ“„ Retrieved {len(result['source_documents'])} matching document(s)")
        print("\nğŸ¤– Answer:\n", result['result'])

        print("\nğŸ“š Sources:")
        for doc in result['source_documents']:
            metadata = doc.metadata
            print(f"- {metadata.get('source', 'Unknown')} ({metadata.get('role', 'Unknown')})")

if __name__ == "__main__":
    main()
